{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.3-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/jonathan/.cache/pypoetry/virtualenvs/analitics-engineering-zk9oOB0Y-py3.12/lib/python3.12/site-packages (24.2)\n",
      "Collecting pip\n",
      "  Using cached pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Using cached pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-24.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/29 10:02:13 WARN Utils: Your hostname, lenovoHome resolves to a loopback address: 127.0.1.1; using 192.168.0.16 instead (on interface wlp1s0)\n",
      "24/11/29 10:02:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/29 10:02:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name| id|\n",
      "+---------+---+\n",
      "|    Alice|  1|\n",
      "|      Bob|  2|\n",
      "|Catherine|  3|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializa a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()\n",
    "\n",
    "# Cria uma lista de tuplas\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Catherine\", 3)]\n",
    "\n",
    "# Define o esquema das colunas\n",
    "columns = [\"name\", \"id\"]\n",
    "\n",
    "# Cria o DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Mostra as primeiras linhas do DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 requests-2.32.3 urllib3-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jonathan/.cache/pypoetry/virtualenvs/analitics-engineering-zk9oOB0Y-py3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/jonathan/.cache/pypoetry/virtualenvs/analitics-engineering-zk9oOB0Y-py3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.1.3 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/29 10:08:57 WARN Utils: Your hostname, lenovoHome resolves to a loopback address: 127.0.1.1; using 192.168.0.16 instead (on interface wlp1s0)\n",
      "24/11/29 10:08:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/29 10:08:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import pandas as pd \n",
    "from pyspark.sql import SparkSession \n",
    "from io import StringIO\n",
    "\n",
    "spark = SparkSession.builder.appName(\"airbnb\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/29 10:09:10 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# URL do arquivos CSV \n",
    "url_listings = 'https://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2024-06-27/data/listings.csv'\n",
    "url_calendar = 'https://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2024-06-27/data/calendar.csv'\n",
    "url_reviews = 'https://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2024-06-27/data/reviews.csv' \n",
    "\n",
    "# Fazer o download do conteúdo do CSV \n",
    "response_listings = requests.get(url_listings) \n",
    "response_listings.raise_for_status() # Verifica se houve algum erro na requisição \n",
    "\n",
    "response_calendar = requests.get(url_calendar) \n",
    "response_calendar.raise_for_status()\n",
    "\n",
    "response_reviews = requests.get(url_reviews) \n",
    "response_reviews.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement distutils (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for distutils\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install distutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/29 10:12:57 WARN TaskSetManager: Stage 1 contains a task of very large size (3794 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= LISTINGS =================\n",
      "Linhas: 34664, Colunas: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/29 10:18:56 WARN TaskSetManager: Stage 4 contains a task of very large size (49367 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/29 10:18:57 ERROR Inbox: An error happened while processing message in the inbox for LocalSchedulerBackendEndpoint\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3540)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:132)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1899)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1808)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1205)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:358)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:530)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:494)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager$$Lambda/0x00007dac5fbc46a0.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:470)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:414)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda/0x00007dac5fbbe490.apply(Unknown Source)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda/0x00007dac5fbbddd8.apply$mcVI$sp(Unknown Source)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:399)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:606)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda/0x00007dac5fbbda00.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:574)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda/0x00007dac5fbbd628.apply(Unknown Source)\n",
      "Exception in thread \"dispatcher-event-loop-2\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3540)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:132)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1899)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1808)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1205)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:358)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:530)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:494)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager$$Lambda/0x00007dac5fbc46a0.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:470)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:414)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda/0x00007dac5fbbe490.apply(Unknown Source)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda/0x00007dac5fbbddd8.apply$mcVI$sp(Unknown Source)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:399)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:606)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda/0x00007dac5fbbda00.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:574)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda/0x00007dac5fbbd628.apply(Unknown Source)\n",
      "ERROR:root:KeyboardInterrupt while sending command.                (0 + 2) / 12]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jonathan/.cache/pypoetry/virtualenvs/analitics-engineering-zk9oOB0Y-py3.12/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonathan/.cache/pypoetry/virtualenvs/analitics-engineering-zk9oOB0Y-py3.12/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m calendar_bronze \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(StringIO(response_calendar\u001b[38;5;241m.\u001b[39mtext))\n\u001b[1;32m     11\u001b[0m calendar_bronze \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(calendar_bronze)\n\u001b[0;32m---> 12\u001b[0m calendar_bronze_linhas \u001b[38;5;241m=\u001b[39m \u001b[43mcalendar_bronze\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m calendar_bronze_colunas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(calendar_bronze\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m================= CALENDAR =================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/analitics-engineering-zk9oOB0Y-py3.12/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/analitics-engineering-zk9oOB0Y-py3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/analitics-engineering-zk9oOB0Y-py3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/analitics-engineering-zk9oOB0Y-py3.12/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                         (0 + 2) / 12]\r"
     ]
    }
   ],
   "source": [
    "# Criar o dataframe Listings\n",
    "listings_bronze = pd.read_csv(StringIO(response_listings.text))\n",
    "listings_bronze = spark.createDataFrame(listings_bronze)\n",
    "listings_bronze_linhas = listings_bronze.count()\n",
    "listings_bronze_colunas = len(listings_bronze.columns)\n",
    "print(\"================= LISTINGS =================\")\n",
    "print(f\"Linhas: {listings_bronze_linhas}, Colunas: {listings_bronze_colunas}\")\n",
    "\n",
    "# Criar o dataframe Calendar\n",
    "calendar_bronze = pd.read_csv(StringIO(response_calendar.text))\n",
    "calendar_bronze = spark.createDataFrame(calendar_bronze)\n",
    "calendar_bronze_linhas = calendar_bronze.count()\n",
    "calendar_bronze_colunas = len(calendar_bronze.columns)\n",
    "print(\"================= CALENDAR =================\")\n",
    "print(f\"Linhas: {calendar_bronze_linhas}, Colunas: {calendar_bronze_colunas}\")\n",
    "\n",
    "# Criar o dataframe Reviews\n",
    "reviews_bronze = pd.read_csv(StringIO(response_reviews.text))\n",
    "reviews_bronze = spark.createDataFrame(reviews_bronze)\n",
    "reviews_bronze_linhas = reviews_bronze.count()\n",
    "reviews_bronze_colunas = len(reviews_bronze.columns)\n",
    "print(\"================= REVIEWS =================\")\n",
    "print(f\"Linhas: {reviews_bronze_linhas}, Colunas: {reviews_bronze_colunas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools\n",
      "  Downloading setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Downloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "Successfully installed setuptools-75.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/29 10:11:56 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/11/29 10:11:57 WARN TaskSetManager: Stage 0 contains a task of very large size (3794 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------+------------+-----------+--------------------+--------------------+---------------------+--------------------+-------+--------------------+--------------------+----------+--------------------+--------------------+------------------+------------------+--------------------+-----------------+--------------------+--------------------+------------------+-------------------+-------------------------+------------------+--------------------+----------------------+--------------------+----------------------+----------------------------+---------+---------+--------------------+---------------+------------+---------+--------------+--------+----+--------------------+-------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+----------------------+------------+-----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-------+----------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+\n",
      "|    id|         listing_url|     scrape_id|last_scraped|     source|                name|         description|neighborhood_overview|         picture_url|host_id|            host_url|           host_name|host_since|       host_location|          host_about|host_response_time|host_response_rate|host_acceptance_rate|host_is_superhost|  host_thumbnail_url|    host_picture_url|host_neighbourhood|host_listings_count|host_total_listings_count|host_verifications|host_has_profile_pic|host_identity_verified|       neighbourhood|neighbourhood_cleansed|neighbourhood_group_cleansed| latitude|longitude|       property_type|      room_type|accommodates|bathrooms|bathrooms_text|bedrooms|beds|           amenities|  price|minimum_nights|maximum_nights|minimum_minimum_nights|maximum_minimum_nights|minimum_maximum_nights|maximum_maximum_nights|minimum_nights_avg_ntm|maximum_nights_avg_ntm|calendar_updated|has_availability|availability_30|availability_60|availability_90|availability_365|calendar_last_scraped|number_of_reviews|number_of_reviews_ltm|number_of_reviews_l30d|first_review|last_review|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|license|instant_bookable|calculated_host_listings_count|calculated_host_listings_count_entire_homes|calculated_host_listings_count_private_rooms|calculated_host_listings_count_shared_rooms|reviews_per_month|\n",
      "+------+--------------------+--------------+------------+-----------+--------------------+--------------------+---------------------+--------------------+-------+--------------------+--------------------+----------+--------------------+--------------------+------------------+------------------+--------------------+-----------------+--------------------+--------------------+------------------+-------------------+-------------------------+------------------+--------------------+----------------------+--------------------+----------------------+----------------------------+---------+---------+--------------------+---------------+------------+---------+--------------+--------+----+--------------------+-------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+----------------------+------------+-----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-------+----------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+\n",
      "| 17878|https://www.airbn...|20240627045056|  2024-06-28|city scrape|Very Nice 2Br in ...|Please note that ...| This is the one o...|https://a0.muscac...|  68997|https://www.airbn...|            Matthias|2010-01-08|Rio de Janeiro, B...|I  am a  journali...|    within an hour|              100%|                100%|                t|https://a0.muscac...|https://a0.muscac...|        Copacabana|                  2|                        5|['email', 'phone']|                   t|                     t|Rio de Janeiro, B...|            Copacabana|                         NaN|-22.96599| -43.1794|        Entire condo|Entire home/apt|           5|      1.0|        1 bath|     2.0| 2.0|[\"Smoking allowed...|$310.00|             5|            28|                     5|                     5|                    28|                    28|                   5.0|                  28.0|             NaN|               t|              6|             32|             51|             274|           2024-06-28|              319|                   23|                     1|  2010-07-15| 2024-06-08|                4.71|                  4.77|                     4.64|                 4.84|                       4.91|                  4.77|               4.67|    NaN|               f|                             1|                                          1|                                           0|                                          0|             1.88|\n",
      "| 25026|https://www.airbn...|20240627045056|  2024-06-28|city scrape|Beautiful Modern ...|**Fully renovated...| Copacabana is a l...|https://a0.muscac...| 102840|https://www.airbn...|             Viviane|2010-04-03|Rio de Janeiro, B...|Hi guys,\\n\\nVivia...|    within an hour|              100%|                 82%|                t|https://a0.muscac...|https://a0.muscac...|        Copacabana|                  1|                        5|['email', 'phone']|                   t|                     t|Rio de Janeiro, B...|            Copacabana|                         NaN|-22.97735|-43.19105|  Entire rental unit|Entire home/apt|           3|      1.0|        1 bath|     1.0| 2.0|[\"Fast wifi \\u201...|$203.00|             2|            60|                     2|                     2|                    60|                    60|                   2.0|                  60.0|             NaN|               t|             24|             54|             71|             241|           2024-06-28|              291|                   23|                     1|  2010-06-07| 2024-06-02|                4.73|                  4.71|                     4.79|                 4.82|                       4.92|                  4.84|               4.62|    NaN|               f|                             1|                                          1|                                           0|                                          0|              1.7|\n",
      "|220377|https://www.airbn...|20240627045056|  2024-06-27|city scrape|Suíte Casal (banh...|The apartment is ...| Tijuca is a resid...|https://a0.muscac...|1142424|https://www.airbn...|             Taciana|2011-09-11|                 NaN|                 NaN|    within an hour|              100%|                 47%|                f|https://a0.muscac...|https://a0.muscac...|            Tijuca|                  3|                        3|         ['phone']|                   t|                     t|Rio de Janeiro, B...|                Tijuca|                         NaN| -22.9288|-43.24046|Private room in r...|   Private room|           2|      1.0|1 private bath|     1.0| 1.0|[\"Essentials\", \"A...|$220.00|             1|           365|                     1|                     1|                  1125|                  1125|                   1.0|                1125.0|             NaN|               t|              9|             29|             41|             283|           2024-06-27|                5|                    1|                     0|  2011-11-08| 2024-03-28|                 5.0|                   5.0|                      5.0|                  5.0|                        4.8|                   4.8|                5.0|    NaN|               f|                             3|                                          0|                                           3|                                          0|             0.03|\n",
      "| 35764|https://www.airbn...|20240627045056|  2024-06-28|city scrape|COPACABANA SEA BR...|Our newly renovat...| Our guests will e...|https://a0.muscac...| 153691|https://www.airbn...|Patricia Miranda ...|2010-06-27|Rio de Janeiro, B...|Hello,   We are P...|within a few hours|              100%|                 98%|                t|https://a0.muscac...|https://a0.muscac...|        Copacabana|                  1|                        2|['email', 'phone']|                   t|                     t|Rio de Janeiro, B...|            Copacabana|                         NaN|-22.98107|-43.19136|         Entire loft|Entire home/apt|           2|      1.5|     1.5 baths|     1.0| 1.0|[\"Essentials\", \"H...|$201.00|             3|            15|                     3|                     4|                    15|                    15|                   3.0|                  15.0|             NaN|               t|              5|             10|             24|              93|           2024-06-28|              476|                   36|                     5|  2010-10-03| 2024-06-25|                4.91|                  4.94|                     4.92|                 4.97|                       4.95|                  4.94|               4.89|    NaN|               f|                             1|                                          1|                                           0|                                          0|             2.85|\n",
      "|223073|https://www.airbn...|20240627045056|  2024-06-28|city scrape|Modern Loft 1 • I...|READ THE FREQUENT...| DON'T KNOW IPANEM...|https://a0.muscac...| 503995|https://www.airbn...|          ❤️ BrUx ❤️|2011-04-12|Rio de Janeiro, B...|Portuguese/Englis...|    within an hour|              100%|                100%|                t|https://a0.muscac...|https://a0.muscac...|           Ipanema|                  7|                        8|['email', 'phone']|                   t|                     t|Rio de Janeiro, B...|               Ipanema|                         NaN| -22.9828|-43.20467|         Entire loft|Entire home/apt|           2|      1.0|        1 bath|     1.0| 1.0|[\"Fast wifi \\u201...|$321.00|             1|           730|                     5|                     9|                  1125|                  1125|                   5.4|                1125.0|             NaN|               t|             13|             43|             73|             348|           2024-06-28|              471|                   17|                     1|  2011-10-06| 2024-06-02|                4.81|                  4.83|                     4.74|                 4.95|                       4.98|                  4.97|               4.73|    NaN|               t|                             7|                                          7|                                           0|                                          0|             3.04|\n",
      "+------+--------------------+--------------+------------+-----------+--------------------+--------------------+---------------------+--------------------+-------+--------------------+--------------------+----------+--------------------+--------------------+------------------+------------------+--------------------+-----------------+--------------------+--------------------+------------------+-------------------+-------------------------+------------------+--------------------+----------------------+--------------------+----------------------+----------------------------+---------+---------+--------------------+---------------+------------+---------+--------------+--------+----+--------------------+-------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+----------------------+------------+-----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-------+----------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_bronze.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analitics-engineering-zk9oOB0Y-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
